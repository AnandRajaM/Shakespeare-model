{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference : https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath =r\"C:\\Users\\hp\\Desktop\\ML Models\\RNNs\\shakespeare.txt\"\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(shakespeare_text[:148])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) #char_level=True means that we are tokenizing characters instead of words\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6, 9, 8, 3]]\n",
      "['f i r s t']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences([\"First\"]))\n",
    "print(tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1115394\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) #total number of distinct characters\n",
    "dataset_size = tokenizer.document_count #total number of characters\n",
    "print(max_id, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  5  8 ... 20 26 10]\n"
     ]
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # we substract 1 to get IDs from 0 to 38 rather than from 1 to 39 becuase we will use the IDs as targets, and we want them to start from 0\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 90% of dataset as training data\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "#above \"from_tensor_slices\"  creates a seperate tensor dataset from each element of the tensor encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat().window(101,shift=1,drop_remainder=True)   \n",
    "#repeat() method basically continues the data until the window is filled\n",
    "#exammple: if data is [1,2,3] and window is 4 then repeat() will make it [1,2,3,1] [2,3,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(101))\n",
    " #Itâ€™s a nested dataset, analogous to a list of lists. However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So, we must call the flat_map() method: it converts a nested dataset into a flat dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout is a regularization technique where a certain proportion of randomly selected neurons are \"dropped out\" \n",
    "#or ignored during training. This means that their contribution to the next layer is temporarily removed.\n",
    "# in this case the drouput rate is 0.2 (its normaly between 0-1) \n",
    "\n",
    "#Recurrent Dropout is a variant of dropout specifically designed for recurrent neural networks. \n",
    "#In RNNs like GRUs or LSTMs, dropout is applied not only to the input units of a layer but also to \n",
    "#the recurrent connections within the layer.\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will take a while to run\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                   epochs=10)\n",
    "model.save(\"shakespeare.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 311ms/step\n",
      "[[3.51145832e-06 1.67668315e-06 3.36331104e-05 1.06268180e-05\n",
      "  3.22712071e-06 1.95957838e-07 2.18989044e-07 1.37377788e-06\n",
      "  3.51481722e-05 1.63527249e-04 1.04154836e-07 1.24610847e-06\n",
      "  9.37553182e-08 9.99730766e-01 1.96858991e-07 6.05944379e-08\n",
      "  1.56148633e-06 8.94588794e-08 8.60609362e-09 2.82524759e-09\n",
      "  2.90034950e-06 4.94311223e-08 1.00237652e-09 3.15139630e-08\n",
      "  4.75855501e-07 1.21086180e-06 1.59099443e-07 1.66274447e-07\n",
      "  2.29161188e-08 1.34984489e-07 1.76165056e-08 1.62302086e-07\n",
      "  2.41419906e-09 4.21676472e-10 7.44343379e-06 3.57352425e-10\n",
      "  1.12415849e-11 1.74161710e-13 1.26173215e-13]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(r\"C:\\Users\\hp\\Desktop\\ML Models\\RNNs\\shakespeare.h5\")\n",
    "\n",
    "# creating functions to preprocvess like earlier for inputs\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "print(model.predict(X_new)[0, -1:, :]) \n",
    "Y_pred = model.predict(X_new)\n",
    "print(tokenizer.sequences_to_texts(Y_pred.argmax(axis=-1) + 1)[0][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])   #text preprocesss , convert to one hot encoding\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]    #predicting the next character classes\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature  #rescaling the logits\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=250, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.5, 1.0, 1.2,1.4]\n",
    "para_list = []\n",
    "for temp in temperatures:\n",
    "    print(f\"temperature: {temp}\")\n",
    "    para_list.append((complete_text(\"Thy\", temperature=temp),temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Thy will accomped\n",
      "the proper in the charge of such a sight of the belly.\n",
      "\n",
      "menenius:\n",
      "you must not give me quarrel.\n",
      "\n",
      "first citizen:\n",
      "what can you see the old and common and be to\n",
      "the corminions of the finger and be this common and suffering of the worst.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.0\n",
      "Thy good to the gater,\n",
      "where he for you can speak the motitutes was he,\n",
      "are these forthy counternon's look in, where company me the store none whereof they,\n",
      "for the one still sign trot with answer,\n",
      "and i say you you studding ovind, well shall be\n",
      "well-bo\n",
      "\n",
      "\n",
      "1.2\n",
      "Thy. this citizens vince\n",
      "hear myself and wares 'that speil was to!\n",
      "\n",
      "staten:\n",
      "sir, humbly, they? when some is give\n",
      "which yet to provide her belly; have gells.\n",
      "i pray you, as i reserve as friends,\n",
      "alone liking, what is the sleve; rock assist you\n",
      "live in hi\n",
      "\n",
      "\n",
      "1.4\n",
      "Thy wiss!\n",
      "\n",
      "menenius:\n",
      "if you have appear fit: go die i hus\n",
      "aursel. nay, what, you shall give toe. forbeon, goe will\n",
      "if i will barve an over at;\n",
      "hortensio, thou thou'rt boy, asl, i'll tell\n",
      "uput a thingf in the rah, theramoind more\n",
      "ever done, and nothing p\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "para_list = para_list.copy()\n",
    "for para,temp in para_list:\n",
    "    print(temp)\n",
    "    print(para)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature set to 1 or 1.2 would be the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3483/3483 [==============================] - 1271s 365ms/step - loss: 1.6047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.604728102684021"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(encoded[train_size:])\n",
    "test_dataset = test_dataset.window(101, shift=1, drop_remainder=True)\n",
    "test_dataset = test_dataset.flat_map(lambda window: window.batch(101))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "test_dataset = test_dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
